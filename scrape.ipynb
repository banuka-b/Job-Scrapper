{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d26ca48d-847a-4d9e-88cf-8e35625a1cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\ww\\onedrive\\desktop\\scrape-jobs\\env\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ww\\onedrive\\desktop\\scrape-jobs\\env\\lib\\site-packages (4.13.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\ww\\onedrive\\desktop\\scrape-jobs\\env\\lib\\site-packages (2.3.0)\n",
      "Collecting openpyxl\n",
      "  Using cached openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ww\\onedrive\\desktop\\scrape-jobs\\env\\lib\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ww\\onedrive\\desktop\\scrape-jobs\\env\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ww\\onedrive\\desktop\\scrape-jobs\\env\\lib\\site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ww\\onedrive\\desktop\\scrape-jobs\\env\\lib\\site-packages (from requests) (2025.4.26)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ww\\onedrive\\desktop\\scrape-jobs\\env\\lib\\site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\ww\\onedrive\\desktop\\scrape-jobs\\env\\lib\\site-packages (from beautifulsoup4) (4.14.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\ww\\onedrive\\desktop\\scrape-jobs\\env\\lib\\site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ww\\onedrive\\desktop\\scrape-jobs\\env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ww\\onedrive\\desktop\\scrape-jobs\\env\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ww\\onedrive\\desktop\\scrape-jobs\\env\\lib\\site-packages (from pandas) (2025.2)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Using cached et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ww\\onedrive\\desktop\\scrape-jobs\\env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Using cached et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   ---------------------------------------- 2/2 [openpyxl]\n",
      "\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4 pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4577b48d-198c-4a08-90ae-52ac917a5a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91fdc5b6-d7c4-4ebc-b008-0e02e2ae88e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to scrape TopJobs.lk...\n",
      "Successfully fetched the webpage\n",
      "=== DEBUGGING INFO ===\n",
      "Found 68 total links\n",
      "Found 47 potential job links\n",
      "\n",
      "First 3 job links found:\n",
      "1. Text: 'IT-Sware/DB/QA/Web/Graphics/GIS' | Href: 'vacancybyfunctionalarea.jsp?FA=SDQ&jst=OPEN'\n",
      "   Parent tag: td\n",
      "   Row content: IT-Sware/DB/QA/Web/Graphics/GISIT-HWare/Networks/SystemsAccounting/Auditing/FinanceBanking & Finance...\n",
      "2. Text: 'IT-HWare/Networks/Systems' | Href: 'vacancybyfunctionalarea.jsp?FA=HNS&jst=OPEN'\n",
      "   Parent tag: td\n",
      "   Row content: IT-Sware/DB/QA/Web/Graphics/GISIT-HWare/Networks/SystemsAccounting/Auditing/FinanceBanking & Finance...\n",
      "3. Text: 'Accounting/Auditing/Finance' | Href: 'vacancybyfunctionalarea.jsp?FA=ACA&jst=OPEN'\n",
      "   Parent tag: td\n",
      "   Row content: IT-Sware/DB/QA/Web/Graphics/GISIT-HWare/Networks/SystemsAccounting/Auditing/FinanceBanking & Finance...\n",
      "\n",
      "=== METHOD 1: Processing vacancy links ===\n",
      "Job 1: IT-Sware/DB/QA/Web/Graphics/GIS at IT-HWare/Networks/Systems\n",
      "Job 2: IT-HWare/Networks/Systems at IT-Sware/DB/QA/Web/Graphics/GIS\n",
      "Job 3: Accounting/Auditing/Finance at IT-Sware/DB/QA/Web/Graphics/GIS\n",
      "\n",
      "Total jobs extracted: 32\n",
      "After removing duplicates: 32 jobs\n",
      "Job data saved to Excel file: topjobs_listings_20250606_210256.xlsx\n",
      "\n",
      "--- Scraping Summary ---\n",
      "Total jobs found: 32\n",
      "Unique companies: 14\n",
      "\n",
      "--- Sample Jobs ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Job Link</th>\n",
       "      <th>Scraped Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IT-Sware/DB/QA/Web/Graphics/GIS</td>\n",
       "      <td>IT-HWare/Networks/Systems</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>https://www.topjobs.lkvacancybyfunctionalarea....</td>\n",
       "      <td>2025-06-06 21:02:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IT-HWare/Networks/Systems</td>\n",
       "      <td>IT-Sware/DB/QA/Web/Graphics/GIS</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>https://www.topjobs.lkvacancybyfunctionalarea....</td>\n",
       "      <td>2025-06-06 21:02:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Accounting/Auditing/Finance</td>\n",
       "      <td>IT-Sware/DB/QA/Web/Graphics/GIS</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>https://www.topjobs.lkvacancybyfunctionalarea....</td>\n",
       "      <td>2025-06-06 21:02:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Banking &amp; Finance/Insurance</td>\n",
       "      <td>IT-Sware/DB/QA/Web/Graphics/GIS</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>https://www.topjobs.lkvacancybyfunctionalarea....</td>\n",
       "      <td>2025-06-06 21:02:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sales/Marketing/Merchandising</td>\n",
       "      <td>IT-Sware/DB/QA/Web/Graphics/GIS</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>https://www.topjobs.lkvacancybyfunctionalarea....</td>\n",
       "      <td>2025-06-06 21:02:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HR/Training</td>\n",
       "      <td>Corporate Management/Analysts</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>https://www.topjobs.lkvacancybyfunctionalarea....</td>\n",
       "      <td>2025-06-06 21:02:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Corporate Management/Analysts</td>\n",
       "      <td>HR/Training</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>https://www.topjobs.lkvacancybyfunctionalarea....</td>\n",
       "      <td>2025-06-06 21:02:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Office Admin/Secretary/Receptionist</td>\n",
       "      <td>HR/Training</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>https://www.topjobs.lkvacancybyfunctionalarea....</td>\n",
       "      <td>2025-06-06 21:02:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Civil Eng/Interior Design/Architecture</td>\n",
       "      <td>HR/Training</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>https://www.topjobs.lkvacancybyfunctionalarea....</td>\n",
       "      <td>2025-06-06 21:02:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>IT-Telecoms</td>\n",
       "      <td>HR/Training</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>https://www.topjobs.lkvacancybyfunctionalarea....</td>\n",
       "      <td>2025-06-06 21:02:56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Job Title                          Company  \\\n",
       "0         IT-Sware/DB/QA/Web/Graphics/GIS        IT-HWare/Networks/Systems   \n",
       "1               IT-HWare/Networks/Systems  IT-Sware/DB/QA/Web/Graphics/GIS   \n",
       "2             Accounting/Auditing/Finance  IT-Sware/DB/QA/Web/Graphics/GIS   \n",
       "3             Banking & Finance/Insurance  IT-Sware/DB/QA/Web/Graphics/GIS   \n",
       "4           Sales/Marketing/Merchandising  IT-Sware/DB/QA/Web/Graphics/GIS   \n",
       "5                             HR/Training    Corporate Management/Analysts   \n",
       "6           Corporate Management/Analysts                      HR/Training   \n",
       "7     Office Admin/Secretary/Receptionist                      HR/Training   \n",
       "8  Civil Eng/Interior Design/Architecture                      HR/Training   \n",
       "9                             IT-Telecoms                      HR/Training   \n",
       "\n",
       "        Location                                           Job Link  \\\n",
       "0  Not specified  https://www.topjobs.lkvacancybyfunctionalarea....   \n",
       "1  Not specified  https://www.topjobs.lkvacancybyfunctionalarea....   \n",
       "2  Not specified  https://www.topjobs.lkvacancybyfunctionalarea....   \n",
       "3  Not specified  https://www.topjobs.lkvacancybyfunctionalarea....   \n",
       "4  Not specified  https://www.topjobs.lkvacancybyfunctionalarea....   \n",
       "5  Not specified  https://www.topjobs.lkvacancybyfunctionalarea....   \n",
       "6  Not specified  https://www.topjobs.lkvacancybyfunctionalarea....   \n",
       "7  Not specified  https://www.topjobs.lkvacancybyfunctionalarea....   \n",
       "8  Not specified  https://www.topjobs.lkvacancybyfunctionalarea....   \n",
       "9  Not specified  https://www.topjobs.lkvacancybyfunctionalarea....   \n",
       "\n",
       "          Scraped Date  \n",
       "0  2025-06-06 21:02:56  \n",
       "1  2025-06-06 21:02:56  \n",
       "2  2025-06-06 21:02:56  \n",
       "3  2025-06-06 21:02:56  \n",
       "4  2025-06-06 21:02:56  \n",
       "5  2025-06-06 21:02:56  \n",
       "6  2025-06-06 21:02:56  \n",
       "7  2025-06-06 21:02:56  \n",
       "8  2025-06-06 21:02:56  \n",
       "9  2025-06-06 21:02:56  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Companies by Job Count:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Company\n",
       "IT-Sware/DB/QA/Web/Graphics/GIS        4\n",
       "HR/Training                            4\n",
       "Customer Relations/Public Relations    4\n",
       "Agriculture/Dairy/Environment          4\n",
       "Supervision/Quality Control            4\n",
       "Hotel/Restaurant/Hospitality           4\n",
       "Logistics/Warehouse/Transport          1\n",
       "Corporate Management/Analysts          1\n",
       "IT-HWare/Networks/Systems              1\n",
       "Travel/Tourism                         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def scrape_topjobs():\n",
    "    \"\"\"\n",
    "    Scrape job listings from TopJobs.lk and save to Excel file\n",
    "    \"\"\"\n",
    "    url = \"https://www.topjobs.lk/applicant/vacancybyfunctionalarea.jsp?FA=IT\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\"\n",
    "    }\n",
    "    \n",
    "    print(\"Starting to scrape TopJobs.lk...\")\n",
    "    \n",
    "    try:\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "        response = requests.get(url, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        print(\"Successfully fetched the webpage\")\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the page: {e}\")\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # Let's debug what we're getting\n",
    "    print(\"=== DEBUGGING INFO ===\")\n",
    "    \n",
    "    # Check for different types of links\n",
    "    all_links = soup.find_all('a')\n",
    "    vacancy_links = [link for link in all_links if link.get('href') and ('vacancy' in link.get('href', '') or 'job' in link.get('href', ''))]\n",
    "    print(f\"Found {len(all_links)} total links\")\n",
    "    print(f\"Found {len(vacancy_links)} potential job links\")\n",
    "    \n",
    "    # Check the first few vacancy links\n",
    "    if vacancy_links:\n",
    "        print(\"\\nFirst 3 job links found:\")\n",
    "        for i, link in enumerate(vacancy_links[:3]):\n",
    "            print(f\"{i+1}. Text: '{link.text.strip()}' | Href: '{link.get('href')}'\")\n",
    "            # Check parent elements\n",
    "            parent = link.parent\n",
    "            if parent:\n",
    "                print(f\"   Parent tag: {parent.name}\")\n",
    "                if parent.name == 'td':\n",
    "                    row = parent.parent\n",
    "                    if row and row.name == 'tr':\n",
    "                        print(f\"   Row content: {row.get_text(strip=True)[:100]}...\")\n",
    "    \n",
    "    # Try different approaches to find job data\n",
    "    job_list = []\n",
    "    \n",
    "    # Method 1: Look for all links with 'vacancy' in href\n",
    "    print(\"\\n=== METHOD 1: Processing vacancy links ===\")\n",
    "    for link in vacancy_links:\n",
    "        try:\n",
    "            job_title = link.text.strip()\n",
    "            if not job_title or len(job_title) < 3:\n",
    "                continue\n",
    "                \n",
    "            # Get the table row containing this link\n",
    "            current_element = link\n",
    "            row = None\n",
    "            \n",
    "            # Traverse up to find the table row\n",
    "            while current_element and current_element.name != 'tr':\n",
    "                current_element = current_element.parent\n",
    "                if current_element is None:\n",
    "                    break\n",
    "            \n",
    "            row = current_element\n",
    "            \n",
    "            if row:\n",
    "                # Look for company name in the same row\n",
    "                company_element = None\n",
    "                \n",
    "                # Try different approaches to find company name\n",
    "                fonts = row.find_all('font')\n",
    "                for font in fonts:\n",
    "                    if font.get('class') == ['mini'] or 'mini' in str(font.get('class', [])):\n",
    "                        company_element = font\n",
    "                        break\n",
    "                \n",
    "                # If no company found with 'mini' class, try other fonts\n",
    "                if not company_element and fonts:\n",
    "                    # Look for font elements that are not the job title\n",
    "                    for font in fonts:\n",
    "                        font_text = font.get_text(strip=True)\n",
    "                        if font_text and font_text != job_title and len(font_text) > 2:\n",
    "                            company_element = font\n",
    "                            break\n",
    "                \n",
    "                # Try looking for company in table cells\n",
    "                if not company_element:\n",
    "                    cells = row.find_all('td')\n",
    "                    for cell in cells:\n",
    "                        cell_text = cell.get_text(strip=True)\n",
    "                        if cell_text and cell_text != job_title and len(cell_text) > 2 and not cell_text.startswith('http'):\n",
    "                            company_element = cell\n",
    "                            break\n",
    "                \n",
    "                if company_element:\n",
    "                    company_name = company_element.get_text(strip=True)\n",
    "                    \n",
    "                    # Build job link\n",
    "                    job_link = link.get('href', '')\n",
    "                    if job_link and not job_link.startswith('http'):\n",
    "                        job_link = f\"https://www.topjobs.lk{job_link}\"\n",
    "                    \n",
    "                    # Look for location info\n",
    "                    row_text = row.get_text()\n",
    "                    location = \"Not specified\"\n",
    "                    sri_lankan_cities = [\"Colombo\", \"Kandy\", \"Galle\", \"Negombo\", \"Matara\", \"Kurunegala\", \"Ratnapura\", \"Badulla\", \"Anuradhapura\", \"Trincomalee\"]\n",
    "                    \n",
    "                    for city in sri_lankan_cities:\n",
    "                        if city in row_text:\n",
    "                            location = city\n",
    "                            break\n",
    "                    \n",
    "                    job_data = {\n",
    "                        \"Job Title\": job_title,\n",
    "                        \"Company\": company_name,\n",
    "                        \"Location\": location,\n",
    "                        \"Job Link\": job_link,\n",
    "                        \"Scraped Date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    }\n",
    "                    \n",
    "                    job_list.append(job_data)\n",
    "                    \n",
    "                    # Show first few jobs being processed\n",
    "                    if len(job_list) <= 3:\n",
    "                        print(f\"Job {len(job_list)}: {job_title} at {company_name}\")\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing job: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nTotal jobs extracted: {len(job_list)}\")\n",
    "    \n",
    "    if not job_list:\n",
    "        print(\"\\n=== DEBUGGING: Let's examine the page structure ===\")\n",
    "        \n",
    "        # Show some sample table rows\n",
    "        tables = soup.find_all('table')\n",
    "        print(f\"Found {len(tables)} tables\")\n",
    "        \n",
    "        for i, table in enumerate(tables[:3]):\n",
    "            rows = table.find_all('tr')\n",
    "            print(f\"\\nTable {i+1} has {len(rows)} rows\")\n",
    "            if rows:\n",
    "                for j, row in enumerate(rows[:3]):\n",
    "                    print(f\"  Row {j+1}: {row.get_text(strip=True)[:150]}...\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(job_list)\n",
    "    \n",
    "    # Remove duplicates based on job title and company\n",
    "    df = df.drop_duplicates(subset=['Job Title', 'Company'], keep='first')\n",
    "    print(f\"After removing duplicates: {len(df)} jobs\")\n",
    "    \n",
    "    # Generate filename with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    excel_filename = f\"topjobs_listings_{timestamp}.xlsx\"\n",
    "    \n",
    "    try:\n",
    "        # Save to Excel\n",
    "        with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:\n",
    "            df.to_excel(writer, sheet_name='Job Listings', index=False)\n",
    "            \n",
    "            workbook = writer.book\n",
    "            worksheet = writer.sheets['Job Listings']\n",
    "            \n",
    "            # Auto-adjust column widths\n",
    "            for column in worksheet.columns:\n",
    "                max_length = 0\n",
    "                column_letter = column[0].column_letter\n",
    "                for cell in column:\n",
    "                    try:\n",
    "                        if len(str(cell.value)) > max_length:\n",
    "                            max_length = len(str(cell.value))\n",
    "                    except:\n",
    "                        pass\n",
    "                adjusted_width = min(max_length + 2, 50)\n",
    "                worksheet.column_dimensions[column_letter].width = adjusted_width\n",
    "        \n",
    "        print(f\"Job data saved to Excel file: {excel_filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to Excel: {e}\")\n",
    "        df.to_csv(f\"topjobs_listings_{timestamp}.csv\", index=False)\n",
    "        print(f\"Job data saved to CSV file instead\")\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"\\n--- Scraping Summary ---\")\n",
    "    print(f\"Total jobs found: {len(df)}\")\n",
    "    print(f\"Unique companies: {df['Company'].nunique()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run the scraper\n",
    "df = scrape_topjobs()\n",
    "\n",
    "# Display the results in Jupyter\n",
    "if df is not None:\n",
    "    print(\"\\n--- Sample Jobs ---\")\n",
    "    display(df.head(10))  # Show first 10 jobs\n",
    "    \n",
    "    # You can also create visualizations\n",
    "    print(f\"\\nTop 10 Companies by Job Count:\")\n",
    "    company_counts = df['Company'].value_counts().head(10)\n",
    "    display(company_counts)\n",
    "else:\n",
    "    print(\"No jobs were scraped. Check the debugging output above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a235085-a694-4eb2-b2ae-88ecdd4605af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
